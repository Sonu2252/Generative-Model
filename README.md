Image and Text or simply the given description of those images are quite difficult to comprehend, as text is an unstructured time series data. To simply put it in a prospect, a simple image might consist of many things, that is mountains, or even children playing in the background or even the cars in the far vicinity. I have come up with an idea to generate description of contents of the image, from the images itself. For this we could use attention mechanism, where a particular area of the input sequence is given more attention than other particular areas. This particular area of high importance is calculated by the dot products between the two vector embeddings (in case of cross attention), and learning about this area is done by iterative processes. Dot products are then usually scaled up and passesd through a softmax function to obtain the attention weights, representing the context of the particular token with the vector embeddings of images
